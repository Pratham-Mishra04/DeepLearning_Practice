{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download oddrationale/mnist-in-csv\n",
        "! unzip mnist-in-csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liORA_tWKHZP",
        "outputId": "3b2da5ce-4b6d-43e1-89e1-f4ab1a4074ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv\n",
            "License(s): CC0-1.0\n",
            "Downloading mnist-in-csv.zip to /content\n",
            " 72% 11.0M/15.2M [00:00<00:00, 56.4MB/s]\n",
            "100% 15.2M/15.2M [00:00<00:00, 72.8MB/s]\n",
            "Archive:  mnist-in-csv.zip\n",
            "  inflating: mnist_test.csv          \n",
            "  inflating: mnist_train.csv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW2KxWK8YItv",
        "outputId": "20dba54f-1671-4882-e8f8-f57fe204824c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "HqvuDVXgLXdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_config = {\n",
        "  \"im_path\": 'mnist'\n",
        "}\n",
        "\n",
        "diffusion_config = {\n",
        "  \"num_timesteps\" : 1000,\n",
        "  \"beta_start\" : 0.0001,\n",
        "  \"beta_end\" : 0.02,\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "  \"im_channels\" : 1,\n",
        "  \"im_size\" : 28,\n",
        "  \"down_channels\" : [32, 64, 128, 256],\n",
        "  \"mid_channels\" : [256, 256, 128],\n",
        "  \"down_sample\" : [True, True, False],\n",
        "  \"time_emb_dim\" : 128,\n",
        "  \"num_down_layers\" : 2,\n",
        "  \"num_mid_layers\" : 2,\n",
        "  \"num_up_layers\" : 2,\n",
        "  \"num_heads\" : 4,\n",
        "}\n",
        "\n",
        "train_config = {\n",
        "  \"ckpt_path\": 'drive/MyDrive/ddpm/model_checkpoints',\n",
        "  \"imgs_path\": 'drive/MyDrive/ddpm/learning_images',\n",
        "  \"batch_size\": 64,\n",
        "  \"num_epochs\": 40,\n",
        "  \"num_samples\" : 10,\n",
        "  \"num_grid_rows\" : 10,\n",
        "  \"lr\": 0.0001,\n",
        "  \"ckpt_name\": 'ddpm_ckpt.pth',\n",
        "}"
      ],
      "metadata": {
        "id": "RldysCb-LYkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create image directory\n",
        "if not os.path.exists(dataset_config['im_path']):\n",
        "  os.mkdir(dataset_config['im_path'])"
      ],
      "metadata": {
        "id": "qz-8HSOCPYUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "dOvPk5ffkJwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the Dataset"
      ],
      "metadata": {
        "id": "yHexguv5IkG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import _csv as csv\n",
        "\n",
        "def extract_images(save_dir, csv_fname):\n",
        "    assert os.path.exists(save_dir), \"Directory {} to save images does not exist\".format(save_dir)\n",
        "    assert os.path.exists(csv_fname), \"Csv file {} does not exist\".format(csv_fname)\n",
        "    with open(csv_fname) as f:\n",
        "        reader = csv.reader(f)\n",
        "        for idx, row in enumerate(reader):\n",
        "            if idx == 0:\n",
        "                continue\n",
        "            im = np.zeros((784))\n",
        "            im[:] = list(map(int, row[1:]))\n",
        "            im = im.reshape((28,28))\n",
        "            if not os.path.exists(os.path.join(save_dir, row[0])):\n",
        "                os.mkdir(os.path.join(save_dir, row[0]))\n",
        "            cv2.imwrite(os.path.join(save_dir, row[0], '{}.png'.format(idx)), im)\n",
        "            if idx % 10000 == 0:\n",
        "                print('Finished creating {} images in {}'.format(idx+1, save_dir))\n",
        "\n",
        "extract_images(dataset_config['im_path'], 'mnist_train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnheAA7bKLS-",
        "outputId": "a84fb2b3-402f-4696-b0f9-31402feb843e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished creating 10001 images in mnist\n",
            "Finished creating 20001 images in mnist\n",
            "Finished creating 30001 images in mnist\n",
            "Finished creating 40001 images in mnist\n",
            "Finished creating 50001 images in mnist\n",
            "Finished creating 60001 images in mnist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiGNQ7jbIUp8"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "    r\"\"\"\n",
        "    Nothing special here. Just a simple dataset class for mnist images.\n",
        "    Created a dataset class rather using torchvision to allow\n",
        "    replacement with any other image dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, split, im_path, im_ext='png'):\n",
        "        r\"\"\"\n",
        "        Init method for initializing the dataset properties\n",
        "        :param split: train/test to locate the image files\n",
        "        :param im_path: root folder of images\n",
        "        :param im_ext: image extension. assumes all\n",
        "        images would be this type.\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        self.im_ext = im_ext\n",
        "        self.images, self.labels = self.load_images(im_path)\n",
        "\n",
        "    def load_images(self, im_path):\n",
        "        r\"\"\"\n",
        "        Gets all images from the path specified\n",
        "        and stacks them all up\n",
        "        :param im_path:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
        "        ims = []\n",
        "        labels = []\n",
        "        for d_name in tqdm(os.listdir(im_path)):\n",
        "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
        "                ims.append(fname)\n",
        "                labels.append(int(d_name))\n",
        "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
        "        return ims, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im = Image.open(self.images[index])\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "\n",
        "        # Convert input to -1 to 1 range.\n",
        "        im_tensor = (2 * im_tensor) - 1\n",
        "        return im_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = MnistDataset('', im_path=dataset_config['im_path'])\n",
        "mnist_loader = DataLoader(mnist, batch_size=train_config['batch_size'], shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzEqWQbiMYeA",
        "outputId": "459cde18-3cba-4b98-b375-674693a22c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 75.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 60000 images for split \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model"
      ],
      "metadata": {
        "id": "jyR1BJFxNHYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Noise Scheduler"
      ],
      "metadata": {
        "id": "FmwqaiJCNJDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class LinearNoiseScheduler:\n",
        "    r\"\"\"\n",
        "    Class for the linear noise scheduler that is used in DDPM.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
        "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
        "\n",
        "    def add_noise(self, original, noise, t):\n",
        "        r\"\"\"\n",
        "        Forward method for diffusion\n",
        "        :param original: Image on which noise is to be applied\n",
        "        :param noise: Random Noise Tensor (from normal dist)\n",
        "        :param t: timestep of the forward process of shape -> (B,)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        original_shape = original.shape\n",
        "        batch_size = original_shape[0]\n",
        "\n",
        "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
        "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
        "\n",
        "        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n",
        "        for _ in range(len(original_shape) - 1):\n",
        "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
        "        for _ in range(len(original_shape) - 1):\n",
        "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
        "\n",
        "        # Apply and Return Forward process equation\n",
        "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
        "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
        "\n",
        "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
        "        r\"\"\"\n",
        "            Use the noise prediction by model to get\n",
        "            xt-1 using xt and the noise predicted\n",
        "        :param xt: current timestep sample\n",
        "        :param noise_pred: model noise prediction\n",
        "        :param t: current timestep we are at\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n",
        "              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n",
        "        x0 = torch.clamp(x0, -1., 1.)\n",
        "\n",
        "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n",
        "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
        "\n",
        "        if t == 0:\n",
        "            return mean, x0\n",
        "        else:\n",
        "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n",
        "            variance = variance * self.betas.to(xt.device)[t]\n",
        "            sigma = variance ** 0.5\n",
        "            z = torch.randn(xt.shape).to(xt.device)\n",
        "\n",
        "            # OR\n",
        "            # variance = self.betas[t]\n",
        "            # sigma = variance ** 0.5\n",
        "            # z = torch.randn(xt.shape).to(xt.device)\n",
        "            return mean + sigma * z, x0"
      ],
      "metadata": {
        "id": "lHHn_Gs_Mtok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
        "                                     beta_start=diffusion_config['beta_start'],\n",
        "                                     beta_end=diffusion_config['beta_end'])"
      ],
      "metadata": {
        "id": "Cp3Hbf9dN7so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def visualize_forward_process(dataset_loader, noise_scheduler, image_index=0, num_steps=100, step_interval=50):\n",
        "    # Get a sample image from the dataset\n",
        "    dataset_iter = iter(dataset_loader)\n",
        "    images = next(dataset_iter)  # We only get one value: the images\n",
        "\n",
        "    image = images[image_index].unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Create a random noise tensor with the same shape as the image\n",
        "    noise = torch.randn_like(image)\n",
        "\n",
        "    # Set up the figure for visualization, calculating how many rows and columns for the grid\n",
        "    timesteps_to_show = range(0, num_steps, step_interval)\n",
        "    num_images = len(timesteps_to_show)\n",
        "    grid_cols = 5\n",
        "    grid_rows = (num_images + grid_cols - 1) // grid_cols  # Calculate the number of rows needed\n",
        "\n",
        "    # Create a list to hold the images\n",
        "    grid_images = []\n",
        "\n",
        "    # Add the original image as the first image in the grid\n",
        "    grid_images.append(image.squeeze(0))\n",
        "\n",
        "    # Add noisy images at each selected timestep\n",
        "    for t in timesteps_to_show:\n",
        "        noisy_image = noise_scheduler.add_noise(image, noise, t)\n",
        "        grid_images.append(noisy_image.squeeze(0))\n",
        "\n",
        "    # Make the grid of images\n",
        "    grid_image = make_grid(grid_images, nrow=grid_cols, padding=5)\n",
        "\n",
        "    # Plot the grid of images\n",
        "    plt.figure(figsize=(12, grid_rows * 2))\n",
        "    plt.imshow(grid_image.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Forward Process: Noising at {num_steps} Steps')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RuVwv1y7Rm74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_index = 0  # Index of the image you want to visualize\n",
        "num_steps = 450\n",
        "\n",
        "visualize_forward_process(mnist_loader, scheduler, image_index=image_index, num_steps=num_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "lvoAJ4lpRrrG",
        "outputId": "55c56a90-8e4e-4e7c-c75d-46fa46b650a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the U-NET Model"
      ],
      "metadata": {
        "id": "9fV10wAnNdWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def get_time_embedding(time_steps, temb_dim):\n",
        "    r\"\"\"\n",
        "    Convert time steps tensor into an embedding using the\n",
        "    sinusoidal time embedding formula\n",
        "    :param time_steps: 1D tensor of length batch size\n",
        "    :param temb_dim: Dimension of the embedding\n",
        "    :return: BxD embedding representation of B time steps\n",
        "    \"\"\"\n",
        "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
        "\n",
        "    # factor = 10000^(2i/d_model)\n",
        "    factor = 10000 ** ((torch.arange(\n",
        "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
        "    )\n",
        "\n",
        "    # pos / factor\n",
        "    # timesteps B -> B, 1 -> B, temb_dim\n",
        "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
        "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
        "    return t_emb"
      ],
      "metadata": {
        "id": "mWesuqBdNVvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Down conv block with attention.\n",
        "    Sequence of following block\n",
        "    1. Resnet block with time embedding\n",
        "    2. Attention block\n",
        "    3. Downsample using 2x2 average pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
        "                 down_sample=True, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.down_sample = down_sample\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [nn.GroupNorm(8, out_channels)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
        "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            # Resnet block of Unet\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            # Attention block of Unet\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "        out = self.down_sample_conv(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "JdhGdISSNsuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MidBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Mid conv block with attention.\n",
        "    Sequence of following blocks\n",
        "    1. Resnet block with time embedding\n",
        "    2. Attention block\n",
        "    3. Resnet block with time embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers + 1)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [nn.GroupNorm(8, out_channels)\n",
        "                for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                for _ in range(num_layers)]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        out = x\n",
        "\n",
        "        # First resnet block\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first[0](out)\n",
        "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second[0](out)\n",
        "        out = out + self.residual_input_conv[0](resnet_input)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            # Attention Block\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "            # Resnet Block\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i+1](out)\n",
        "            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i+1](out)\n",
        "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "115Aq_cRNx4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Up conv block with attention.\n",
        "    Sequence of following blocks\n",
        "    1. Upsample\n",
        "    1. Concatenate Down block output\n",
        "    2. Resnet block with time embedding\n",
        "    3. Attention Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.up_sample = up_sample\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [\n",
        "                nn.GroupNorm(8, out_channels)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [\n",
        "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
        "                                                 4, 2, 1) \\\n",
        "            if self.up_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, out_down, t_emb):\n",
        "        x = self.up_sample_conv(x)\n",
        "        x = torch.cat([x, out_down], dim=1)\n",
        "\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "mJcZRllUN3WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    r\"\"\"\n",
        "    Unet model comprising\n",
        "    Down blocks, Midblocks and Uplocks\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config):\n",
        "        super().__init__()\n",
        "        im_channels = model_config['im_channels']\n",
        "        self.down_channels = model_config['down_channels']\n",
        "        self.mid_channels = model_config['mid_channels']\n",
        "        self.t_emb_dim = model_config['time_emb_dim']\n",
        "        self.down_sample = model_config['down_sample']\n",
        "        self.num_down_layers = model_config['num_down_layers']\n",
        "        self.num_mid_layers = model_config['num_mid_layers']\n",
        "        self.num_up_layers = model_config['num_up_layers']\n",
        "\n",
        "        assert self.mid_channels[0] == self.down_channels[-1]\n",
        "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
        "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
        "\n",
        "        # Initial projection from sinusoidal time embedding\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "        )\n",
        "\n",
        "        self.up_sample = list(reversed(self.down_sample))\n",
        "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        for i in range(len(self.down_channels)-1):\n",
        "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n",
        "                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers))\n",
        "\n",
        "        self.mids = nn.ModuleList([])\n",
        "        for i in range(len(self.mid_channels)-1):\n",
        "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n",
        "                                      num_layers=self.num_mid_layers))\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for i in reversed(range(len(self.down_channels)-1)):\n",
        "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n",
        "                                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers))\n",
        "\n",
        "        self.norm_out = nn.GroupNorm(8, 16)\n",
        "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
        "        # Shapes assuming midblocks are [C4, C4, C3]\n",
        "        # Shapes assuming downsamples are [True, True, False]\n",
        "        # B x C x H x W\n",
        "        out = self.conv_in(x)\n",
        "        # B x C1 x H x W\n",
        "\n",
        "        # t_emb -> B x t_emb_dim\n",
        "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
        "        t_emb = self.t_proj(t_emb)\n",
        "\n",
        "        down_outs = []\n",
        "\n",
        "        for idx, down in enumerate(self.downs):\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t_emb)\n",
        "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
        "        # out B x C4 x H/4 x W/4\n",
        "\n",
        "        for mid in self.mids:\n",
        "            out = mid(out, t_emb)\n",
        "        # out B x C3 x H/4 x W/4\n",
        "\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            out = up(out, down_out, t_emb)\n",
        "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
        "        out = self.norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.conv_out(out)\n",
        "        # out B x C x H x W\n",
        "        return out"
      ],
      "metadata": {
        "id": "vOC48W_YN5ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "KcmTUV-7OF7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "1cgQiBVVOOZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(model_config).to(device)"
      ],
      "metadata": {
        "id": "kVuNtsP1OI7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyZySDS-Ol3T",
        "outputId": "70a55687-b509-4a3d-845b-aca3921ded0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unet(\n",
              "  (t_proj): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (1): SiLU()\n",
              "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  )\n",
              "  (conv_in): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (downs): ModuleList(\n",
              "    (0): DownBlock(\n",
              "      (resnet_conv_first): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (t_emb_layers): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnet_conv_second): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attention_norms): ModuleList(\n",
              "        (0-1): 2 x GroupNorm(8, 64, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (residual_input_conv): ModuleList(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (down_sample_conv): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "    (1): DownBlock(\n",
              "      (resnet_conv_first): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (t_emb_layers): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnet_conv_second): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attention_norms): ModuleList(\n",
              "        (0-1): 2 x GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (residual_input_conv): ModuleList(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (down_sample_conv): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "    (2): DownBlock(\n",
              "      (resnet_conv_first): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (t_emb_layers): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnet_conv_second): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attention_norms): ModuleList(\n",
              "        (0-1): 2 x GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (residual_input_conv): ModuleList(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (down_sample_conv): Identity()\n",
              "    )\n",
              "  )\n",
              "  (mids): ModuleList(\n",
              "    (0): MidBlock(\n",
              "      (resnet_conv_first): ModuleList(\n",
              "        (0-2): 3 x Sequential(\n",
              "          (0): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (t_emb_layers): ModuleList(\n",
              "        (0-2): 3 x Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnet_conv_second): ModuleList(\n",
              "        (0-2): 3 x Sequential(\n",
              "          (0): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attention_norms): ModuleList(\n",
              "        (0-1): 2 x GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (residual_input_conv): ModuleList(\n",
              "        (0-2): 3 x Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (1): MidBlock(\n",
              "      (resnet_conv_first): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1-2): 2 x Sequential(\n",
              "          (0): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (t_emb_layers): ModuleList(\n",
              "        (0-2): 3 x Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnet_conv_second): ModuleList(\n",
              "        (0-2): 3 x Sequential(\n",
              "          (0): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attention_norms): ModuleList(\n",
              "        (0-1): 2 x GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (residual_input_conv): ModuleList(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1-2): 2 x Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ups): ModuleList(\n",
              "    (0): UpBlock(\n",
              "      (resnet_conv_first): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (t_emb_layers): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnet_conv_second): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attention_norms): ModuleList(\n",
              "        (0-1): 2 x GroupNorm(8, 64, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (residual_input_conv): ModuleList(\n",
              "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (up_sample_conv): Identity()\n",
              "    )\n",
              "    (1): UpBlock(\n",
              "      (resnet_conv_first): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (t_emb_layers): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=128, out_features=32, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnet_conv_second): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attention_norms): ModuleList(\n",
              "        (0-1): 2 x GroupNorm(8, 32, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (residual_input_conv): ModuleList(\n",
              "        (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (up_sample_conv): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "    (2): UpBlock(\n",
              "      (resnet_conv_first): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (t_emb_layers): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnet_conv_second): ModuleList(\n",
              "        (0-1): 2 x Sequential(\n",
              "          (0): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (attention_norms): ModuleList(\n",
              "        (0-1): 2 x GroupNorm(8, 16, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (residual_input_conv): ModuleList(\n",
              "        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (up_sample_conv): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (norm_out): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
              "  (conv_out): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directories\n",
        "if not os.path.exists(train_config['ckpt_path']):\n",
        "  os.mkdir(train_config['ckpt_path'])\n",
        "\n",
        "# Load checkpoint if found\n",
        "if os.path.exists(os.path.join(train_config['ckpt_path'],train_config['ckpt_name'])):\n",
        "  print('Loading checkpoint as found one')\n",
        "  model.load_state_dict(torch.load(os.path.join(train_config['ckpt_path'],\n",
        "                                                 train_config['ckpt_name']), map_location=device))"
      ],
      "metadata": {
        "id": "YOtZMmglOnyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04779995-1319-44f2-e641-6e461d082b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint as found one\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-b149f0767d11>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(os.path.join(train_config['ckpt_path'],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "# Specify training parameters\n",
        "num_epochs = train_config['num_epochs']\n",
        "optimizer = Adam(model.parameters(), lr=train_config['lr'])\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "cOOpaXgcOq3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def save_sample(model, scheduler, epoch_idx):\n",
        "    r\"\"\"\n",
        "    Sample stepwise by going backward one timestep at a time.\n",
        "    We save the final x0 predictions (after the last timestep).\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode for sampling\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations for sampling\n",
        "        xt = torch.randn((train_config['num_samples'],\n",
        "                          model_config['im_channels'],\n",
        "                          model_config['im_size'],\n",
        "                          model_config['im_size'])).to(device)\n",
        "\n",
        "        for i in tqdm(reversed(range(diffusion_config['num_timesteps']))):\n",
        "            # Predict noise at this timestep\n",
        "            noise_pred = model(xt, torch.tensor([i], device=device))\n",
        "\n",
        "            # Get xt-1 and predicted x0\n",
        "            xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.tensor(i).to(device))\n",
        "\n",
        "            # Save the predicted x0 only at the final step (timestep i == 0)\n",
        "            if i == 0:\n",
        "                ims = torch.clamp(x0_pred, -1., 1.).cpu()  # Clamp to valid range\n",
        "                ims = (ims + 1) / 2  # Normalize to [0, 1]\n",
        "                grid = make_grid(ims, nrow=train_config['num_grid_rows'])\n",
        "                img = torchvision.transforms.ToPILImage()(grid)\n",
        "\n",
        "                # Create samples directory if it doesn't exist\n",
        "                sample_dir = train_config['imgs_path']\n",
        "                os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "                img.save(os.path.join(sample_dir, f'x0_epoch{epoch_idx+1}_final.png'))\n",
        "                img.close()\n",
        "\n",
        "    model.train()  # Set model back to training mode after sampling"
      ],
      "metadata": {
        "id": "Zo0o6FG5Y6HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch_idx in range(7, num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for im in tqdm(mnist_loader):\n",
        "        optimizer.zero_grad()\n",
        "        im = im.float().to(device)\n",
        "\n",
        "        # Sample random noise\n",
        "        noise = torch.randn_like(im).to(device)\n",
        "\n",
        "        # Sample timestep\n",
        "        t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n",
        "\n",
        "        # Add noise to images according to timestep\n",
        "        noisy_im = scheduler.add_noise(im, noise, t)\n",
        "        noise_pred = model(noisy_im, t)\n",
        "\n",
        "        loss = criterion(noise_pred, noise)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print epoch loss\n",
        "    print('Finished epoch:{} | Loss : {:.4f}'.format(\n",
        "        epoch_idx + 1,\n",
        "        np.mean(losses),\n",
        "    ))\n",
        "\n",
        "    # Save model checkpoint\n",
        "    torch.save(model.state_dict(), os.path.join(train_config['ckpt_path'],\n",
        "                                                train_config['ckpt_name']))\n",
        "\n",
        "    # Generate and save sample images every 5 epochs\n",
        "    if (epoch_idx + 1) % 5 == 0:\n",
        "        save_sample(model, scheduler, epoch_idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "wegGDqokO4kq",
        "outputId": "61ce2889-7b8e-49ef-dc1d-89b601bfb509"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/938 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 938/938 [05:34<00:00,  2.81it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:8 | Loss : 0.0246\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:9 | Loss : 0.0249\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:10 | Loss : 0.0248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [00:26, 38.01it/s]\n",
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:11 | Loss : 0.0249\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:33<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:12 | Loss : 0.0246\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:13 | Loss : 0.0248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:14 | Loss : 0.0248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:15 | Loss : 0.0247\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [00:25, 39.20it/s]\n",
            "100%|██████████| 938/938 [05:31<00:00,  2.83it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:16 | Loss : 0.0249\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:31<00:00,  2.83it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:17 | Loss : 0.0248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:18 | Loss : 0.0248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:19 | Loss : 0.0247\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:20 | Loss : 0.0248\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [00:25, 39.35it/s]\n",
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:21 | Loss : 0.0250\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [05:32<00:00,  2.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:22 | Loss : 0.0248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 281/938 [01:40<03:54,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-721ee7fc3d95>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Sample random noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Usage"
      ],
      "metadata": {
        "id": "bjnztB61O_Ic"
      }
    }
  ]
}